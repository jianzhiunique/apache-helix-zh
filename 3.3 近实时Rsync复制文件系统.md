## 近实时Rsync复制文件系统

### 快速演示

- 该演示启动了3个ID为 `localhost_12001, localhost_12002, localhost_12003`  `的`实例
- 每个实例将其文件存储在 `/tmp/<id>/filestore`
- `localhost_12001` 被指定为 master,  `localhost_12002` 和 `localhost_12003` 是 slaves
- 写入主服务器的文件会自动复制到从服务器。在此演示中，将a.txt和b.txt写入 `/tmp/localhost_12001/filestore` 并将它们复制到其他文件夹。
- 当主服务器停止时，将, `localhost_12002` 提升为主服务器
- 另一个 slave `localhost_12003` 停止从 `localhost_12001` 复制数据，开始从新 master `localhost_12002`复制数据
- 写入新 master `localhost_12002`的文件被复制到 `localhost_12003`
- 在此快速演示的最终状态中, `localhost_12002` 是 master and `localhost_12003` 是 slave. 在 `/tmp/localhost_12002/filestore` 手动创建文件将会在 `/tmp/localhost_12003/filestore` 出现
- 忽略控制台上被中断的异常:-)

```
git clone https://git-wip-us.apache.org/repos/asf/helix.git
cd helix
git checkout tags/helix-0.9.8
cd recipes/rsync-replicated-file-system/
mvn clean install package -DskipTests
cd target/rsync-replicated-file-system-pkg/bin
chmod +x *
./quickdemo
```

### Overview

有许多需要存储大量相对较小数据文件的应用程序。示例包括用于存储小型视频，图像，邮件附件等的媒体存储。这些对象中的每个对象通常为千字节，通常不大于几兆字节。这些用例的另一个显着特征是，通常仅添加或删除文件，很少更新文件。有更新时，它们没有任何并发要求。

这些要求比通用分布式文件系统必须满足的要简单得多。其中包括对文件的并发访问，对读取和更新的随机访问，posix合规性以及其他。为了满足这些要求，一般的DFS也非常复杂，其构建和维护成本很高。

分布式文件系统的另一种实现方式包括受Google GFS启发的HDFS。这是构成Hadoop的主要数据存储平台的使用最广泛的分布式文件系统之一。HDFS的主要目的是处理非常大的数据集，并通过将文件分割成固定大小的块，从而将文件分布在一系列商品服务器上。HDFS并不是特别适合存储大量相对较小的文件。

### File Store

正如我们所指出的，有可能为具有更简单要求的应用程序类构建一个非常简单的系统。

- 大量文件，但每个文件都相对较小
- 访问仅限于创建，删除和获取整个文件
- 没有更新已创建的文件（或者删除旧文件并创建一个新文件是可行的）

我们将此系统称为分区文件存储（PFS），以将其与其他分布式文件系统区分开。该系统需要提供以下功能：

- CRD访问大量小文件
- 可伸缩性：文件应根据存储要求分布在大量商用服务器上
- 容错：每个文件应在多台服务器上复制，以免个别服务器故障不会降低可用性
- 弹性：应该可以轻松地向集群添加容量

Apache Helix是一个通用的群集管理框架，可以非常轻松地提供可伸缩性，容错性和弹性功能。rsync可以轻松用作服务器之间的复制通道，以便每个文件都可以在多个服务器上复制。

### Design

#### High Level

- 根据文件名对文件系统进行分区
- 任何时间只有master可以写入
- 为了实现冗余，我们需要具有称为slave的其他副本。slave可以选择性提供读取
- Slave 从 master复制数据
- 当master故障时，slave被提升为master

#### Transaction Log

在master 进行的每次写入都会导致一个或多个文件的创建/删除。 为了保持时间轴一致性，slave需要以相同的顺序应用更改。 为此， 将每个事务记录在一个文件中，并且每个事务都与一个64位ID相关联，其中32 LSB代表序列号， MSB 代表 generation number（代） 。序列号在每个代递增，代在新的master被选举时增长。

#### Replication

Slave需要复制才能跟上master, 每次slave应用更改时，它都会检查最后一次应用的事务ID。在重新启动期间，可以从上一次的检查点ID拉取master的改变，与master相似，slave也记录每一次变化到事务日志中，但是他不会使用新的事务ID，而是使用master的ID。

#### Failover

当master发生故障时，新的slave将被提升为master。如果以前的master节点是可访问的，则新的master将在接管master之前清除以前的master的所有更改。新的master将记录当前代的结束最后ID，然后以从1开始的顺序开始新的一代。此后master将开始接受写入。

![Partitioned File Store](http://helix.apache.org/0.9.8-docs/images/PFS-Generic.png)

### Rsync-based Solution

![Rsync based File Store](http://helix.apache.org/0.9.8-docs/images/RSYNC_BASED_PFS.png)

此应用程序演示了使用rsync作为复制机制的文件存储。可以设想一个类似的系统，在该系统中，可以使用一种自定义解决方案来代替更改而不是使用rsync来通知从属设备，还可以提供一个API来提取更改文件。

#### Concepts

- file_store_dir：实际数据文件的根目录
- change_log_dir：事务日志在此文件夹下生成
- check_point_dir：从站将检查点（最后处理的事务）存储在此处

#### Master

- File server: 此组件支持文件上载和下载，并将文件写入`file_store_dir`。这不包括在此应用程序中。这个想法是，大多数应用程序具有实现此组件的不同方法，并具有一些关联的业务逻辑。如果需要的话，提出这样一个组件并不难。
- File store watcher: 此组件监视本地文件系统上的`file_store_dir`目录是否有任何更改，并将更改通知给已注册的侦听器
- Change log generator: 它注册为文件存储监视程序的侦听器，并在每次通知时将更改记录到`change_log_dir`下的文件中

#### Slave

- File server: 此组件仅支持读取
- Cluster state observer: 观察集群状态，并能够知道谁是当前master
- Replicator: 它具有两个子组件
  - Periodic rsync of change log: 这是一个后台进程定期同步master的`change_log_dir`中的更改到其本地目录
  - Change Log Watcher: 监视`change_log_dir`中的更改并将更改通知给已注册的侦听器
  - On demand rsync invoker: 注册为change log watcher的一个监听器，每次改变时调用rsync来同步发生变化的文件

#### Coordination 协调

节点之间的协调由Helix完成。Helix进行分区管理，并根据复制因子将分区分配给多个节点。它选择一个节点作为主节点，将其他节点指定为从节点。它以状态转换的形式(Offline to Slave, Slave to Master).向每个节点提供通知。当集群状态发生变化时，它还会提供通知。这允许slave停止从当前master复制，并开始从新master复制。

在此应用程序中，我们只有一个分区，但是很容易扩展它以支持多个分区。通过对文件存储进行分区，可以添加新的节点，Helix将自动在这些节点之间重新分配分区。总而言之，Helix提供了分区管理，容错能力并促进了自动集群扩展。