# 分布式锁管理器

分布式锁用于同步访问共享资源。如今，大多数应用程序都使用ZooKeeper对分布式锁进行建模。

使用ZooKeeper建模锁的最简单方法是（有关确切和更高级的解决方案，请参见ZooKeeper领导者秘诀）

- 每个过程都尝试创建一个临时节点
- 如果成功创建了节点，则该过程将获取锁
- 否则，它将监视ZNode并尝试在当前锁持有者消失的情况下再次获取该锁。

如果只有一个锁，这就足够了。但是实际上，应用程序将需要许多这样的锁。在差异过程之间分配和管理锁变得具有挑战性。将这种解决方案扩展到许多锁将导致：

- 节点之间的锁分配不均；首先启动的节点将获取所有锁。稍后启动的节点将处于空闲状态。
- 当节点发生故障时，无法确定如何在其余节点之间分配锁。
- 添加新节点时，当前节点不会放弃锁，以便新节点可以获取一些锁

换句话说，我们希望系统满足以下要求。

- 在所有节点之间平均分配锁，以提高硬件利用率
- 如果某个节点发生故障，则该节点获取的锁应在其他节点之间平均分配
- 如果添加了节点，则锁必须在节点之间平均分配。

Helix为这个问题提供了一种简单而优雅的解决方案。只需指定锁的数量，Helix将确保满足上述约束。

要快速查看此工作，请运行`lock-manager-demo`脚本，其中12个锁在三个节点之间平均分配，并且当一个节点发生故障时，锁将在其余两个节点之间重新分配。请注意，Helix不会完全重新调整锁，而是仅将死节点放弃的锁平均分配到其余2个节点中。

------

### Short Version

此版本在同一进程中启动多个线程，以模拟多节点部署。尝试使用长版本，以更好地了解其工作原理。

```
git clone https://git-wip-us.apache.org/repos/asf/helix.git
cd helix
git checkout tags/helix-0.9.8
mvn clean install package -DskipTests
cd recipes/distributed-lock-manager/target/distributed-lock-manager-pkg/bin
chmod +x *
./lock-manager-demo
```

#### Output

```
./lock-manager-demo
STARTING localhost_12000
STARTING localhost_12002
STARTING localhost_12001
STARTED localhost_12000
STARTED localhost_12002
STARTED localhost_12001
localhost_12001 acquired lock:lock-group_3
localhost_12000 acquired lock:lock-group_8
localhost_12001 acquired lock:lock-group_2
localhost_12001 acquired lock:lock-group_4
localhost_12002 acquired lock:lock-group_1
localhost_12002 acquired lock:lock-group_10
localhost_12000 acquired lock:lock-group_7
localhost_12001 acquired lock:lock-group_5
localhost_12002 acquired lock:lock-group_11
localhost_12000 acquired lock:lock-group_6
localhost_12002 acquired lock:lock-group_0
localhost_12000 acquired lock:lock-group_9
lockName    acquired By
======================================
lock-group_0    localhost_12002
lock-group_1    localhost_12002
lock-group_10    localhost_12002
lock-group_11    localhost_12002
lock-group_2    localhost_12001
lock-group_3    localhost_12001
lock-group_4    localhost_12001
lock-group_5    localhost_12001
lock-group_6    localhost_12000
lock-group_7    localhost_12000
lock-group_8    localhost_12000
lock-group_9    localhost_12000
Stopping localhost_12000
localhost_12000 Interrupted
localhost_12001 acquired lock:lock-group_9
localhost_12001 acquired lock:lock-group_8
localhost_12002 acquired lock:lock-group_6
localhost_12002 acquired lock:lock-group_7
lockName    acquired By
======================================
lock-group_0    localhost_12002
lock-group_1    localhost_12002
lock-group_10    localhost_12002
lock-group_11    localhost_12002
lock-group_2    localhost_12001
lock-group_3    localhost_12001
lock-group_4    localhost_12001
lock-group_5    localhost_12001
lock-group_6    localhost_12002
lock-group_7    localhost_12002
lock-group_8    localhost_12001
lock-group_9    localhost_12001
```

------

### Long version

这提供了有关如何设置集群以及在何处插入应用程序代码的更多详细信息。

#### Start ZooKeeper

```
./start-standalone-zookeeper 2199
```

#### Create a Cluster

```
./helix-admin --zkSvr localhost:2199 --addCluster lock-manager-demo
```

#### Create a Lock Group

Create a lock group and specify the number of locks in the lock group.

```
./helix-admin --zkSvr localhost:2199  --addResource lock-manager-demo lock-group 6 OnlineOffline --mode AUTO_REBALANCE
```

#### Start the Nodes

创建一个Lock类来处理回调

```
public class Lock extends StateModel {
  private String lockName;

  public Lock(String lockName) {
    this.lockName = lockName;
  }

  public void lock(Message m, NotificationContext context) {
    System.out.println(" acquired lock:"+ lockName );
  }

  public void release(Message m, NotificationContext context) {
    System.out.println(" releasing lock:"+ lockName );
  }

}
```

LockFactory 来创建锁

```
public class LockFactory extends StateModelFactory<Lock> {
    /* Instantiates the lock handler, one per lockName */
    public Lock create(String lockName) {
        return new Lock(lockName);
    }
}
```

在节点启动时，只需加入集群，helix将会调用合适的回调在合适的锁实例上。可以启动任意数量的节点，helix将探测新的节点加入集群，然后重新自动分布锁

```
public class LockProcess {
  public static void main(String args) {
    String zkAddress= "localhost:2199";
    String clusterName = "lock-manager-demo";
    
    //给每个程序一个唯一ID，常用的格式是hostname_port
    String instanceName ="localhost_12000";
    ZKHelixAdmin helixAdmin = new ZKHelixAdmin(zkAddress);
    
    //配置实例，提供一些元数据
    InstanceConfig config = new InstanceConfig(instanceName);
    config.setHostName("localhost");
    config.setPort("12000");
    admin.addInstance(clusterName, config);
    
    //加入集群
    HelixManager manager;
    manager = HelixManagerFactory.getHelixManager(clusterName,
                                                  instanceName,
                                                  InstanceType.PARTICIPANT,
                                                  zkAddress);
    manager.getStateMachineEngine().registerStateModelFactory("OnlineOffline", modelFactory);
    //这里的modelFactory？
    manager.connect();
    Thread.currentThread.join();
  }
}
```

#### 启动控制器

控制器可以作为单独的进程启动，也可以嵌入每个节点中

##### 单独的进程

当群集中的节点数> 100时，建议使用此选项。为实现容错，可以在不同的机器上运行多个控制器。

```
./run-helix-controller --zkSvr localhost:2199 --cluster lock-manager-demo 2>&1 > /tmp/controller.log &
```

##### 嵌入到节点的程序中

当集群中的节点数少于100个时，建议使用此方法。要从每个进程启动控制器，只需将以下行添加到MyClass中

```
public class LockProcess {
  public static void main(String args) {
    String zkAddress= "localhost:2199";
    String clusterName = "lock-manager-demo";
    // .
    // .
    manager.connect();
    HelixManager controller;
    controller = HelixControllerMain.startHelixController(zkAddress,
                                                          clusterName,
                                                          "controller",
                                                          HelixControllerMain.STANDALONE);
    Thread.currentThread.join();
  }
}
```